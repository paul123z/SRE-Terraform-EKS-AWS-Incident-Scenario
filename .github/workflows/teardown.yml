name: Teardown Infrastructure

on:
  workflow_dispatch:
    inputs:
      confirm_destroy:
        description: 'Type "DESTROY" to confirm infrastructure destruction'
        required: true
        type: string
      skip_verification:
        description: 'Skip post-teardown verification'
        required: false
        default: false
        type: boolean
      force_destroy:
        description: 'Force destroy (ignore errors)'
        required: false
        default: false
        type: boolean

env:
  AWS_REGION: eu-central-1
  CLUSTER_NAME: sre-incident-demo-cluster
  ECR_REPOSITORY: sre-demo-app
  APP_NAME: sre-demo-app
  NAMESPACE: default
  TF_STATE_BUCKET: sre-terraform-state-${{ github.repository_owner }}
  TF_STATE_KEY: sre-demo/terraform.tfstate
  TF_DYNAMODB_TABLE: sre-terraform-locks

jobs:
  confirm-destruction:
    name: ‚ö†Ô∏è Confirm Destruction
    runs-on: ubuntu-latest
    steps:
      - name: Validate Confirmation
        run: |
          if [ "${{ inputs.confirm_destroy }}" != "DESTROY" ]; then
            echo "‚ùå Destruction not confirmed. You must type 'DESTROY' exactly to proceed."
            echo "This safety check prevents accidental infrastructure destruction."
            exit 1
          fi
          echo "‚úÖ Destruction confirmed. Proceeding with teardown..."

      - name: Display Warning
        run: |
          echo "‚ö†Ô∏è ‚ö†Ô∏è ‚ö†Ô∏è WARNING ‚ö†Ô∏è ‚ö†Ô∏è ‚ö†Ô∏è"
          echo ""
          echo "This workflow will PERMANENTLY DESTROY the following AWS resources:"
          echo "- EKS Cluster: ${{ env.CLUSTER_NAME }}"
          echo "- VPC and all networking components"
          echo "- ECR Repository: ${{ env.ECR_REPOSITORY }}"
          echo "- S3 Buckets (incident logs)"
          echo "- Lambda Functions"
          echo "- IAM Roles and Policies"
          echo "- All associated resources"
          echo ""
          echo "üí∞ This action will STOP AWS charges for these resources."
          echo "üìä All monitoring data will be LOST."
          echo "üîÑ You will need to run deploy.yml to recreate the environment."
          echo ""
          echo "‚è≥ Starting teardown in 10 seconds..."
          sleep 10

  pre-teardown-backup:
    name: üíæ Pre-Teardown Backup
    runs-on: ubuntu-latest
    needs: confirm-destruction
    continue-on-error: true
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Backup Current State
        run: |
          echo "üíæ Creating backup of current infrastructure state..."
          
          # Create backup directory
          mkdir -p /tmp/teardown-backup
          
          # Backup EKS cluster info
          if aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "üìä Backing up EKS cluster information..."
            aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} > /tmp/teardown-backup/eks-cluster.json
            aws eks list-nodegroups --cluster-name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} > /tmp/teardown-backup/eks-nodegroups.json
          fi
          
          # Backup ECR repository info
          if aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "üì¶ Backing up ECR repository information..."
            aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} > /tmp/teardown-backup/ecr-repository.json
            aws ecr list-images --repository-name ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} > /tmp/teardown-backup/ecr-images.json 2>/dev/null || echo "No images found"
          fi
          
          # Backup S3 buckets
          echo "üóÇÔ∏è Listing S3 buckets..."
          aws s3api list-buckets --query 'Buckets[?contains(Name, `sre-`) || contains(Name, `terraform-state`)].Name' --output text > /tmp/teardown-backup/s3-buckets.txt
          
          # Backup VPC information
          echo "üåê Backing up VPC information..."
          aws ec2 describe-vpcs --filters "Name=tag:Name,Values=sre-incident-demo-vpc" --region ${{ env.AWS_REGION }} > /tmp/teardown-backup/vpc-info.json 2>/dev/null || echo "VPC not found"
          
          # Backup timestamp
          echo "Backup created at: $(date -u +%Y-%m-%dT%H:%M:%SZ)" > /tmp/teardown-backup/backup-info.txt
          echo "AWS Account: $(aws sts get-caller-identity --query Account --output text)" >> /tmp/teardown-backup/backup-info.txt
          echo "Region: ${{ env.AWS_REGION }}" >> /tmp/teardown-backup/backup-info.txt
          
          echo "‚úÖ Pre-teardown backup completed"

      - name: Upload Backup
        uses: actions/upload-artifact@v4
        with:
          name: pre-teardown-backup-${{ github.run_number }}
          path: /tmp/teardown-backup/
          retention-days: 30

  remove-kubernetes-resources:
    name: ‚ò∏Ô∏è Remove Kubernetes Resources
    runs-on: ubuntu-latest
    needs: [confirm-destruction, pre-teardown-backup]
    continue-on-error: ${{ inputs.force_destroy }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Check EKS Cluster
        id: cluster-check
        run: |
          if aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "cluster-exists=true" >> $GITHUB_OUTPUT
            echo "‚úÖ EKS cluster found"
          else
            echo "cluster-exists=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è EKS cluster not found, skipping Kubernetes cleanup"
          fi

      - name: Configure kubectl
        if: steps.cluster-check.outputs.cluster-exists == 'true'
        run: |
          echo "‚öôÔ∏è Configuring kubectl..."
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: Setup Helm
        if: steps.cluster-check.outputs.cluster-exists == 'true'
        uses: azure/setup-helm@v3
        with:
          version: v3.12.0

      - name: Remove Helm Releases
        if: steps.cluster-check.outputs.cluster-exists == 'true'
        run: |
          echo "üóëÔ∏è Removing Helm releases..."
          
          # Remove application
          if helm list -n ${{ env.NAMESPACE }} | grep -q ${{ env.APP_NAME }}; then
            echo "üóëÔ∏è Uninstalling application..."
            helm uninstall ${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} --wait --timeout 5m
            echo "‚úÖ Application uninstalled"
          else
            echo "‚ÑπÔ∏è Application not found"
          fi
          
          # Remove monitoring stack
          if helm list -n monitoring | grep -q prometheus; then
            echo "üóëÔ∏è Uninstalling monitoring stack..."
            helm uninstall prometheus -n monitoring --wait --timeout 10m
            echo "‚úÖ Monitoring stack uninstalled"
          else
            echo "‚ÑπÔ∏è Monitoring stack not found"
          fi
          
          # Remove EBS CSI Driver
          if helm list -n kube-system | grep -q aws-ebs-csi-driver; then
            echo "üóëÔ∏è Uninstalling EBS CSI Driver..."
            helm uninstall aws-ebs-csi-driver -n kube-system --wait --timeout 5m
            echo "‚úÖ EBS CSI Driver uninstalled"
          else
            echo "‚ÑπÔ∏è EBS CSI Driver not found"
          fi

      - name: Remove Kubernetes Dashboard
        if: steps.cluster-check.outputs.cluster-exists == 'true'
        run: |
          echo "üóëÔ∏è Removing Kubernetes Dashboard..."
          kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml --ignore-not-found=true
          echo "‚úÖ Kubernetes Dashboard removed"

      - name: Remove Metrics Server
        if: steps.cluster-check.outputs.cluster-exists == 'true'
        run: |
          echo "üóëÔ∏è Removing Metrics Server..."
          kubectl delete -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml --ignore-not-found=true
          echo "‚úÖ Metrics Server removed"

      - name: Force Remove Stuck Resources
        if: steps.cluster-check.outputs.cluster-exists == 'true'
        run: |
          echo "üîß Force removing any stuck resources..."
          
          # Remove finalizers from stuck namespaces
          for ns in monitoring; do
            if kubectl get namespace $ns 2>/dev/null | grep -q Terminating; then
              echo "üîß Removing finalizers from namespace $ns..."
              kubectl patch namespace $ns -p '{"metadata":{"finalizers":null}}' --type=merge || true
            fi
          done
          
          # Wait for resources to be cleaned up
          echo "‚è≥ Waiting for resources to be cleaned up..."
          sleep 30

  remove-aws-resources:
    name: ‚òÅÔ∏è Remove AWS Resources
    runs-on: ubuntu-latest
    needs: [confirm-destruction, remove-kubernetes-resources]
    continue-on-error: ${{ inputs.force_destroy }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Clean ECR Repository
        run: |
          echo "üóëÔ∏è Cleaning ECR repository..."
          
          if aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "üì¶ Deleting ECR images..."
            
            # Delete all images
            aws ecr batch-delete-image \
              --repository-name ${{ env.ECR_REPOSITORY }} \
              --image-ids imageTag=latest \
              --region ${{ env.AWS_REGION }} || true
            
            # List and delete any remaining images
            IMAGE_IDS=$(aws ecr list-images --repository-name ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} --query 'imageIds[*]' --output json)
            if [ "$IMAGE_IDS" != "[]" ]; then
              aws ecr batch-delete-image \
                --repository-name ${{ env.ECR_REPOSITORY }} \
                --image-ids "$IMAGE_IDS" \
                --region ${{ env.AWS_REGION }} || true
            fi
            
            # Delete repository
            aws ecr delete-repository \
              --repository-name ${{ env.ECR_REPOSITORY }} \
              --force \
              --region ${{ env.AWS_REGION }}
            
            echo "‚úÖ ECR repository deleted"
          else
            echo "‚ÑπÔ∏è ECR repository not found"
          fi

      - name: Clean S3 Buckets
        run: |
          echo "üóëÔ∏è Cleaning S3 buckets..."
          
          # Get AWS account ID
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          
          # Clean incident logs bucket
          INCIDENT_BUCKET="sre-incident-demo-incident-logs-${AWS_ACCOUNT_ID}"
          if aws s3api head-bucket --bucket "$INCIDENT_BUCKET" 2>/dev/null; then
            echo "üóÇÔ∏è Emptying incident logs bucket..."
            aws s3 rm "s3://$INCIDENT_BUCKET" --recursive || true
            aws s3api delete-bucket --bucket "$INCIDENT_BUCKET" --region ${{ env.AWS_REGION }} || true
            echo "‚úÖ Incident logs bucket deleted"
          else
            echo "‚ÑπÔ∏è Incident logs bucket not found"
          fi

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.8.4
          terraform_wrapper: false

      - name: Configure Terraform Backend for Teardown
        working-directory: terraform
        run: |
          echo "üîß Configuring Terraform backend for teardown..."
          
          # Backup existing backend configuration
          if grep -q "backend.*local" main.tf; then
            echo "üìÅ Found local backend in main.tf, backing up..."
            cp main.tf main.tf.backup
            
            # Remove local backend from main.tf temporarily
            sed -i '/backend "local"/,/}/d' main.tf
            echo "‚úÖ Local backend temporarily removed"
          fi
          
          # Get AWS account ID
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          STATE_BUCKET="${{ env.TF_STATE_BUCKET }}-${AWS_ACCOUNT_ID}"
          
          # Create S3 backend configuration
          cat > backend.tf << EOF
          terraform {
            backend "s3" {
              bucket         = "$STATE_BUCKET"
              key            = "${{ env.TF_STATE_KEY }}"
              region         = "${{ env.AWS_REGION }}"
              dynamodb_table = "${{ env.TF_DYNAMODB_TABLE }}"
              encrypt        = true
            }
          }
          EOF
          
          echo "‚úÖ S3 backend configuration created for teardown"

      - name: Terraform Init
        working-directory: terraform
        run: |
          echo "üîÑ Initializing Terraform..."
          terraform init
          echo "‚úÖ Terraform initialized"

      - name: Terraform Destroy
        working-directory: terraform
        run: |
          echo "üí• Destroying infrastructure with Terraform..."
          
          if [ "${{ inputs.force_destroy }}" = "true" ]; then
            echo "‚ö†Ô∏è Force destroy enabled - ignoring errors"
            terraform destroy -auto-approve || echo "‚ö†Ô∏è Some resources may have failed to destroy"
          else
            terraform destroy -auto-approve
          fi
          
          echo "‚úÖ Terraform destroy completed"

      - name: Restore Original Terraform Configuration
        working-directory: terraform
        if: always()
        run: |
          echo "üîÑ Restoring original Terraform configuration..."
          
          # Remove CI/CD backend configuration
          rm -f backend.tf
          
          # Restore original main.tf if we backed it up
          if [ -f main.tf.backup ]; then
            mv main.tf.backup main.tf
            echo "‚úÖ Original main.tf restored"
          fi
          
          echo "‚úÖ Terraform configuration restored for local development"

      - name: Clean Terraform State Bucket
        run: |
          echo "üóëÔ∏è Cleaning Terraform state bucket..."
          
          # Get AWS account ID
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          STATE_BUCKET="${{ env.TF_STATE_BUCKET }}-${AWS_ACCOUNT_ID}"
          
          # Clean state bucket
          if aws s3api head-bucket --bucket "$STATE_BUCKET" 2>/dev/null; then
            echo "üóÇÔ∏è Emptying Terraform state bucket..."
            aws s3 rm "s3://$STATE_BUCKET" --recursive || true
            
            # Delete the bucket
            aws s3api delete-bucket --bucket "$STATE_BUCKET" --region ${{ env.AWS_REGION }} || true
            echo "‚úÖ Terraform state bucket deleted"
          else
            echo "‚ÑπÔ∏è Terraform state bucket not found"
          fi
          
          # Clean DynamoDB table
          if aws dynamodb describe-table --table-name ${{ env.TF_DYNAMODB_TABLE }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "üóëÔ∏è Deleting DynamoDB lock table..."
            aws dynamodb delete-table --table-name ${{ env.TF_DYNAMODB_TABLE }} --region ${{ env.AWS_REGION }}
            echo "‚úÖ DynamoDB lock table deleted"
          else
            echo "‚ÑπÔ∏è DynamoDB lock table not found"
          fi

  verify-cleanup:
    name: ‚úÖ Verify Cleanup
    runs-on: ubuntu-latest
    needs: [confirm-destruction, remove-aws-resources]
    if: ${{ !inputs.skip_verification }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Run Verification Script
        run: |
          echo "üîç Running teardown verification..."
          
          # Create a simplified verification script
          cat > /tmp/verify-teardown.sh << 'EOF'
          #!/bin/bash
          
          AWS_REGION="${{ env.AWS_REGION }}"
          CLUSTER_NAME="${{ env.CLUSTER_NAME }}"
          ECR_REPOSITORY="${{ env.ECR_REPOSITORY }}"
          
          VERIFICATION_FAILED=false
          
          echo "üîç Verifying EKS cluster removal..."
          if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" 2>/dev/null; then
            echo "‚ùå EKS cluster still exists"
            VERIFICATION_FAILED=true
          else
            echo "‚úÖ EKS cluster not found"
          fi
          
          echo "üîç Verifying ECR repository removal..."
          if aws ecr describe-repositories --repository-names "$ECR_REPOSITORY" --region "$AWS_REGION" 2>/dev/null; then
            echo "‚ùå ECR repository still exists"
            VERIFICATION_FAILED=true
          else
            echo "‚úÖ ECR repository not found"
          fi
          
          echo "üîç Verifying VPC removal..."
          VPC_IDS=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=sre-incident-demo-vpc" --region "$AWS_REGION" --query 'Vpcs[].VpcId' --output text 2>/dev/null || echo "")
          if [ -n "$VPC_IDS" ]; then
            echo "‚ùå Project VPC still exists: $VPC_IDS"
            VERIFICATION_FAILED=true
          else
            echo "‚úÖ Project VPC not found"
          fi
          
          echo "üîç Verifying load balancers removal..."
          LBS=$(aws elbv2 describe-load-balancers --region "$AWS_REGION" --query 'LoadBalancers[?contains(LoadBalancerName, `sre-`) || contains(LoadBalancerName, `k8s-`)].LoadBalancerArn' --output text 2>/dev/null || echo "")
          if [ -n "$LBS" ]; then
            echo "‚ö†Ô∏è Some load balancers may still exist (they should be cleaned up automatically)"
          else
            echo "‚úÖ No project load balancers found"
          fi
          
          if [ "$VERIFICATION_FAILED" = true ]; then
            echo "‚ùå Verification failed - some resources still exist"
            echo "üí° You may need to manually clean up remaining resources"
            echo "üí° Or wait a few minutes and run teardown-verify.yml workflow"
            exit 1
          else
            echo "‚úÖ All resources successfully removed"
          fi
          EOF
          
          chmod +x /tmp/verify-teardown.sh
          /tmp/verify-teardown.sh

  generate-summary:
    name: üìã Generate Summary
    runs-on: ubuntu-latest
    needs: [confirm-destruction, remove-kubernetes-resources, remove-aws-resources, verify-cleanup]
    if: always()
    steps:
      - name: Generate Teardown Summary
        run: |
          echo "## üßπ Infrastructure Teardown Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üéØ Teardown Status" >> $GITHUB_STEP_SUMMARY
          
          # Check job statuses
          if [ "${{ needs.confirm-destruction.result }}" = "success" ]; then
            echo "- ‚úÖ **Confirmation:** Destruction confirmed" >> $GITHUB_STEP_SUMMARY
          else
            echo "- ‚ùå **Confirmation:** Failed or skipped" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.remove-kubernetes-resources.result }}" = "success" ]; then
            echo "- ‚úÖ **Kubernetes Resources:** Successfully removed" >> $GITHUB_STEP_SUMMARY
          else
            echo "- ‚ö†Ô∏è **Kubernetes Resources:** ${{ needs.remove-kubernetes-resources.result }}" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.remove-aws-resources.result }}" = "success" ]; then
            echo "- ‚úÖ **AWS Resources:** Successfully removed" >> $GITHUB_STEP_SUMMARY
          else
            echo "- ‚ö†Ô∏è **AWS Resources:** ${{ needs.remove-aws-resources.result }}" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.verify-cleanup.result }}" = "success" ]; then
            echo "- ‚úÖ **Verification:** All resources confirmed removed" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.verify-cleanup.result }}" = "skipped" ]; then
            echo "- ‚è≠Ô∏è **Verification:** Skipped as requested" >> $GITHUB_STEP_SUMMARY
          else
            echo "- ‚ö†Ô∏è **Verification:** ${{ needs.verify-cleanup.result }}" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üí∞ Cost Impact" >> $GITHUB_STEP_SUMMARY
          echo "- **EKS Cluster:** Billing stopped" >> $GITHUB_STEP_SUMMARY
          echo "- **EC2 Instances:** Terminated" >> $GITHUB_STEP_SUMMARY
          echo "- **Load Balancers:** Removed" >> $GITHUB_STEP_SUMMARY
          echo "- **NAT Gateway:** Deleted" >> $GITHUB_STEP_SUMMARY
          echo "- **Storage:** EBS volumes deleted" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìÅ Backup" >> $GITHUB_STEP_SUMMARY
          echo "- **Pre-teardown backup:** Available in workflow artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- **Retention:** 30 days" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üîÑ Recreation" >> $GITHUB_STEP_SUMMARY
          echo "To recreate the environment:" >> $GITHUB_STEP_SUMMARY
          echo "1. Run the **deploy.yml** workflow" >> $GITHUB_STEP_SUMMARY
          echo "2. Wait for complete deployment (~20-30 minutes)" >> $GITHUB_STEP_SUMMARY
          echo "3. Run **incident-demo.yml** for testing" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ "${{ needs.verify-cleanup.result }}" != "success" ]; then
            echo "### ‚ö†Ô∏è Action Required" >> $GITHUB_STEP_SUMMARY
            echo "Some resources may still exist. Consider:" >> $GITHUB_STEP_SUMMARY
            echo "1. Running **teardown-verify.yml** to check remaining resources" >> $GITHUB_STEP_SUMMARY
            echo "2. Manual cleanup via AWS Console" >> $GITHUB_STEP_SUMMARY
            echo "3. Checking AWS billing for unexpected charges" >> $GITHUB_STEP_SUMMARY
          else
            echo "### ‚úÖ Complete" >> $GITHUB_STEP_SUMMARY
            echo "Infrastructure teardown completed successfully. No further action required." >> $GITHUB_STEP_SUMMARY
          fi
