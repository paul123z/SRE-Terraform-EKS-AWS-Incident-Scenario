name: Incident Simulation Demo

on:
  workflow_dispatch:
    inputs:
      incident_type:
        description: 'Type of incident to simulate'
        required: true
        default: 'memory_leak'
        type: choice
        options:
        - memory_leak
        - cpu_stress
        - health_failure
        - all_scenarios
      duration:
        description: 'Duration in minutes'
        required: true
        default: '1'
        type: string
      enable_ai_analysis:
        description: 'Enable AI incident analysis'
        required: false
        default: true
        type: boolean

env:
  AWS_REGION: eu-central-1
  CLUSTER_NAME: sre-incident-demo-cluster
  APP_NAME: sre-demo-app
  NAMESPACE: default
  TF_STATE_BUCKET: sre-terraform-state-${{ github.repository_owner }}
  TF_STATE_KEY: sre-demo/terraform.tfstate

jobs:
  pre-checks:
    name: üîç Pre-Flight Checks
    runs-on: ubuntu-latest
    outputs:
      service-url: ${{ steps.service.outputs.service-url }}
      cluster-status: ${{ steps.cluster.outputs.status }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Check EKS Cluster
        id: cluster
        run: |
          echo "üîç Checking EKS cluster status..."
          if aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.status' --output text | grep -q "ACTIVE"; then
            echo "status=active" >> $GITHUB_OUTPUT
            echo "‚úÖ EKS cluster is active"
          else
            echo "status=inactive" >> $GITHUB_OUTPUT
            echo "‚ùå EKS cluster is not active"
            exit 1
          fi

      - name: Configure kubectl
        run: |
          echo "‚öôÔ∏è Configuring kubectl..."
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}
          kubectl cluster-info

      - name: Check Application Status
        id: service
        run: |
          echo "üîç Checking application status..."
          
          # Check if pods are running
          if ! kubectl get pods -l app.kubernetes.io/name=${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} --no-headers | grep -q "Running"; then
            echo "‚ùå Application pods are not running"
            kubectl get pods -l app.kubernetes.io/name=${{ env.APP_NAME }} -n ${{ env.NAMESPACE }}
            exit 1
          fi
          
          # Get service URL
          SERVICE_URL=$(kubectl get svc ${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ -z "$SERVICE_URL" ]; then
            SERVICE_URL=$(kubectl get svc ${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
          fi
          
          if [ -n "$SERVICE_URL" ]; then
            echo "service-url=$SERVICE_URL" >> $GITHUB_OUTPUT
            echo "‚úÖ Service URL: http://$SERVICE_URL"
            
            # Test application health
            if curl -s --max-time 10 "http://$SERVICE_URL/health" > /dev/null; then
              echo "‚úÖ Application is responding"
            else
              echo "‚ö†Ô∏è Application may not be fully ready"
            fi
          else
            echo "‚ùå Service URL not available"
            exit 1
          fi

  baseline-metrics:
    name: üìä Capture Baseline Metrics
    runs-on: ubuntu-latest
    needs: pre-checks
    outputs:
      baseline-captured: ${{ steps.metrics.outputs.captured }}
      incident-id: ${{ steps.metrics.outputs.incident-id }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: Capture Baseline Metrics
        id: metrics
        run: |
          echo "üìä Capturing baseline metrics..."
          
          # Create incident directory
          INCIDENT_ID="gh-incident-$(date +%Y%m%d-%H%M%S)"
          mkdir -p /tmp/incident-logs
          
          echo "üÜî Incident ID: $INCIDENT_ID"
          echo "incident-id=$INCIDENT_ID" >> $GITHUB_OUTPUT
          
          # Capture baseline state
          echo "=== BASELINE METRICS ===" > /tmp/incident-logs/baseline-$INCIDENT_ID.log
          echo "Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          echo "Incident ID: $INCIDENT_ID" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          echo "" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          
          # Pod status
          echo "=== POD STATUS ===" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          kubectl get pods -l app.kubernetes.io/name=${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} -o wide >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          echo "" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          
          # Resource usage
          echo "=== RESOURCE USAGE ===" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          kubectl top pods -l app.kubernetes.io/name=${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} >> /tmp/incident-logs/baseline-$INCIDENT_ID.log 2>&1 || echo "Metrics server not available" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          echo "" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          
          # Service status
          echo "=== SERVICE STATUS ===" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          kubectl get svc ${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} -o wide >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          echo "" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          
          # HPA status
          echo "=== HPA STATUS ===" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          kubectl get hpa -n ${{ env.NAMESPACE }} >> /tmp/incident-logs/baseline-$INCIDENT_ID.log 2>&1 || echo "HPA not found" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          echo "" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          
          # Application health
          echo "=== APPLICATION HEALTH ===" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          curl -s --max-time 10 "http://${{ needs.pre-checks.outputs.service-url }}/health" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log 2>&1 || echo "Health check failed" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          echo "" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          
          echo "captured=true" >> $GITHUB_OUTPUT
          echo "‚úÖ Baseline metrics captured"

      - name: Upload Baseline Logs
        uses: actions/upload-artifact@v4
        with:
          name: baseline-logs
          path: /tmp/incident-logs/
          retention-days: 7

  simulate-incident:
    name: üö® Simulate Incident
    runs-on: ubuntu-latest
    needs: [pre-checks, baseline-metrics]
    outputs:
      incident-id: ${{ steps.incident.outputs.incident-id }}
      s3-incident-url: ${{ steps.s3-upload.outputs.s3-incident-url }}
      s3-bucket: ${{ steps.s3-upload.outputs.s3-bucket }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: Download Baseline Logs
        uses: actions/download-artifact@v4
        with:
          name: baseline-logs
          path: /tmp/incident-logs/

      - name: Simulate Incident
        id: incident
        run: |
          echo "üö® Starting incident simulation..."
          
          SERVICE_URL="${{ needs.pre-checks.outputs.service-url }}"
          INCIDENT_TYPE="${{ inputs.incident_type }}"
          DURATION="${{ inputs.duration }}"
          INCIDENT_ID="${{ needs.baseline-metrics.outputs.incident-id }}"
          
          echo "üÜî Incident ID: $INCIDENT_ID"
          echo "üéØ Incident Type: $INCIDENT_TYPE"
          echo "‚è±Ô∏è Duration: $DURATION minutes"
          echo "üåê Service URL: http://$SERVICE_URL"
          
          echo "incident-id=$INCIDENT_ID" >> $GITHUB_OUTPUT
          
          # Create incident log directory
          mkdir -p /tmp/incident-logs
          
          # Start incident logging (create log file in expected format)
          echo "=== INCIDENT SIMULATION LOG ===" > /tmp/incident-logs/incident-$INCIDENT_ID.log
          echo "Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
          echo "Incident ID: $INCIDENT_ID" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
          echo "Incident Type: $INCIDENT_TYPE" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
          echo "Duration: $DURATION minutes" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
          echo "Service URL: http://$SERVICE_URL" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
          echo "" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
          
          # Function to trigger incident
          trigger_incident() {
            local type=$1
            echo "üö® Triggering $type incident..." >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            
            case $type in
              "memory_leak")
                echo "üíæ Enabling memory leak simulation..."
                curl -X POST "http://$SERVICE_URL/api/memory-leak" \
                  -H "Content-Type: application/json" \
                  -d '{"enable": true}' \
                  >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1
                ;;
              "cpu_stress")
                echo "üî• Enabling CPU stress simulation..."
                curl -X POST "http://$SERVICE_URL/api/cpu-stress" \
                  -H "Content-Type: application/json" \
                  -d '{"enable": true}' \
                  >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1
                ;;
              "health_failure")
                echo "üíî Enabling health failure simulation..."
                curl -X POST "http://$SERVICE_URL/api/failure-mode" \
                  -H "Content-Type: application/json" \
                  -d '{"mode": "health_failure"}' \
                  >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1
                ;;
            esac
          }
          
          # Function to stop incident
          stop_incident() {
            echo "üõë Stopping incident simulation..." >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            
            # Stop all simulations
            curl -X POST "http://$SERVICE_URL/api/memory-leak" \
              -H "Content-Type: application/json" \
              -d '{"enable": false}' \
              >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1
            
            curl -X POST "http://$SERVICE_URL/api/cpu-stress" \
              -H "Content-Type: application/json" \
              -d '{"enable": false}' \
              >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1
            
            curl -X POST "http://$SERVICE_URL/api/failure-mode" \
              -H "Content-Type: application/json" \
              -d '{"mode": "none"}' \
              >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1
          }
          
          # Function to collect metrics
          collect_metrics() {
            local timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)
            echo "üìä Collecting metrics at $timestamp..." >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            
            # Pod status
            echo "=== POD STATUS ($timestamp) ===" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            kubectl get pods -l app.kubernetes.io/name=${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} -o wide >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            
            # Resource usage
            echo "=== RESOURCE USAGE ($timestamp) ===" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            kubectl top pods -l app.kubernetes.io/name=${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1 || echo "Metrics server not available" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            
            # Recent events
            echo "=== RECENT EVENTS ($timestamp) ===" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            kubectl get events --sort-by='.lastTimestamp' -n ${{ env.NAMESPACE }} --field-selector involvedObject.name=${{ env.APP_NAME }} | tail -10 >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1
            
            # Application health
            echo "=== APPLICATION HEALTH ($timestamp) ===" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            curl -s --max-time 5 "http://$SERVICE_URL/health" >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1 || echo "Health check failed" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            
            echo "" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
          }
          
          # Trigger incidents based on type
          if [ "$INCIDENT_TYPE" = "all_scenarios" ]; then
            echo "üé≠ Running all incident scenarios..."
            for scenario in "memory_leak" "cpu_stress" "health_failure"; do
              echo "--- Starting $scenario scenario ---" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
              trigger_incident $scenario
              sleep 30
              collect_metrics
              stop_incident
              sleep 10
            done
          else
            trigger_incident $INCIDENT_TYPE
            
            # Monitor for specified duration
            DURATION_SECONDS=$((DURATION * 60))
            INTERVAL=30
            ITERATIONS=$((DURATION_SECONDS / INTERVAL))
            
            echo "üìä Monitoring for $DURATION minutes (collecting metrics every ${INTERVAL}s)..."
            for i in $(seq 1 $ITERATIONS); do
              echo "üìä Metrics collection $i/$ITERATIONS"
              collect_metrics
              
              if [ $i -lt $ITERATIONS ]; then
                sleep $INTERVAL
              fi
            done
            
            # Stop the incident
            stop_incident
          fi
          
          # Final metrics collection
          echo "üìä Collecting final metrics..."
          sleep 10
          collect_metrics
          
          echo "‚úÖ Incident simulation completed"

      - name: Collect Pod Logs
        run: |
          echo "üìã Collecting pod logs..."
          INCIDENT_ID="${{ steps.incident.outputs.incident-id }}"
          
          # Get pod logs
          echo "=== APPLICATION LOGS ===" > /tmp/incident-logs/pod-logs-$INCIDENT_ID.log
          kubectl logs -l app.kubernetes.io/name=${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} --tail=500 >> /tmp/incident-logs/pod-logs-$INCIDENT_ID.log 2>&1 || echo "Failed to collect pod logs" >> /tmp/incident-logs/pod-logs-$INCIDENT_ID.log
          
          echo "‚úÖ Pod logs collected"

      - name: Upload to S3 (if AI analysis enabled)
        id: s3-upload
        if: ${{ inputs.enable_ai_analysis }}
        run: |
          echo "‚òÅÔ∏è Uploading logs to S3 for AI analysis..."
          INCIDENT_ID="${{ steps.incident.outputs.incident-id }}"
          
          # Get AWS account ID
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          S3_BUCKET="sre-incident-demo-incident-logs-${AWS_ACCOUNT_ID}"
          
          # Create S3 bucket if it doesn't exist
          if ! aws s3api head-bucket --bucket "$S3_BUCKET" 2>/dev/null; then
            echo "üì¶ Creating S3 bucket for incident logs..."
            aws s3api create-bucket \
              --bucket "$S3_BUCKET" \
              --region ${{ env.AWS_REGION }} \
              --create-bucket-configuration LocationConstraint=${{ env.AWS_REGION }}
            
            # Enable versioning and encryption
            aws s3api put-bucket-versioning --bucket "$S3_BUCKET" --versioning-configuration Status=Enabled
            aws s3api put-bucket-encryption --bucket "$S3_BUCKET" --server-side-encryption-configuration '{
              "Rules": [{
                "ApplyServerSideEncryptionByDefault": {"SSEAlgorithm": "AES256"},
                "BucketKeyEnabled": true
              }]
            }'
          fi
          
          # Upload logs (check files exist first)
          echo "üîç Checking available log files..."
          ls -la /tmp/incident-logs/ || echo "No log files found"
          
          echo "üì§ Uploading logs to S3..."
          if [ -f "/tmp/incident-logs/incident-$INCIDENT_ID.log" ]; then
            aws s3 cp /tmp/incident-logs/incident-$INCIDENT_ID.log s3://$S3_BUCKET/incidents/$INCIDENT_ID/incident.log
            echo "‚úÖ Incident log uploaded"
          else
            echo "‚ö†Ô∏è Incident log not found: /tmp/incident-logs/incident-$INCIDENT_ID.log"
          fi
          
          if [ -f "/tmp/incident-logs/baseline-$INCIDENT_ID.log" ]; then
            aws s3 cp /tmp/incident-logs/baseline-$INCIDENT_ID.log s3://$S3_BUCKET/incidents/$INCIDENT_ID/baseline.log
            echo "‚úÖ Baseline log uploaded"
          else
            echo "‚ö†Ô∏è Baseline log not found: /tmp/incident-logs/baseline-$INCIDENT_ID.log"
          fi
          
          if [ -f "/tmp/incident-logs/pod-logs-$INCIDENT_ID.log" ]; then
            aws s3 cp /tmp/incident-logs/pod-logs-$INCIDENT_ID.log s3://$S3_BUCKET/incidents/$INCIDENT_ID/pod-logs.log
            echo "‚úÖ Pod logs uploaded"
          else
            echo "‚ö†Ô∏è Pod logs not found: /tmp/incident-logs/pod-logs-$INCIDENT_ID.log"
          fi
          
          # Generate S3 URLs for AI analysis
          S3_INCIDENT_URL="https://$S3_BUCKET.s3.${{ env.AWS_REGION }}.amazonaws.com/incidents/$INCIDENT_ID/incident.log"
          
          echo "s3-bucket=$S3_BUCKET" >> $GITHUB_OUTPUT
          echo "s3-incident-url=$S3_INCIDENT_URL" >> $GITHUB_OUTPUT
          echo "‚úÖ Logs uploaded to S3: s3://$S3_BUCKET/incidents/$INCIDENT_ID/"

      - name: Upload Incident Logs Artifact
        id: upload
        uses: actions/upload-artifact@v4
        with:
          name: incident-logs-${{ steps.incident.outputs.incident-id }}
          path: /tmp/incident-logs/
          retention-days: 7

  ai-analysis:
    name: ü§ñ AI Incident Analysis
    runs-on: ubuntu-latest
    needs: [simulate-incident]
    if: ${{ inputs.enable_ai_analysis }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-1  # Bedrock region

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq curl unzip

      - name: Extract S3 info and download log
        id: s3-download
        run: |
          echo "ü§ñ Running AI incident analysis..."
          INCIDENT_ID="${{ needs.simulate-incident.outputs.incident-id }}"
          S3_INCIDENT_URL="${{ needs.simulate-incident.outputs.s3-incident-url }}"
          S3_BUCKET="${{ needs.simulate-incident.outputs.s3-bucket }}"
          
          echo "üîç Debug info:"
          echo "  Incident ID: '$INCIDENT_ID'"
          echo "  S3 URL: '$S3_INCIDENT_URL'"
          echo "  S3 Bucket: '$S3_BUCKET'"
          
          if [ -z "$S3_INCIDENT_URL" ]; then
            echo "‚ùå ERROR: Missing S3 incident URL from simulate-incident job!"
            echo "Available outputs:"
            echo "  incident-id: '${{ needs.simulate-incident.outputs.incident-id }}'"
            echo "  s3-incident-url: '${{ needs.simulate-incident.outputs.s3-incident-url }}'"
            echo "  s3-bucket: '${{ needs.simulate-incident.outputs.s3-bucket }}'"
            exit 1
          fi
          
          if [ -z "$INCIDENT_ID" ]; then
            echo "‚ö†Ô∏è WARNING: Incident ID is empty, using timestamp fallback"
            INCIDENT_ID="fallback-$(date +%Y%m%d-%H%M%S)"
          fi
          
          # Extract S3 bucket and key from URL
          if [[ "$S3_INCIDENT_URL" == https://*.s3.*.amazonaws.com/* ]]; then
            BUCKET=$(echo "$S3_INCIDENT_URL" | sed 's|https://||' | cut -d'.' -f1)
            KEY=$(echo "$S3_INCIDENT_URL" | sed 's|https://[^/]*/||')
          else
            echo "‚ùå ERROR: Invalid S3 URL format: $S3_INCIDENT_URL"
            exit 1
          fi
          
          echo "üì• Downloading incident log from S3..."
          echo "  Bucket: $BUCKET"
          echo "  Key: $KEY"
          
          aws s3 cp "s3://$BUCKET/$KEY" ./incident-log.log
          
          echo "üìä Log file info:"
          echo "  Size: $(wc -c < incident-log.log) bytes"
          echo "  Lines: $(wc -l < incident-log.log)"
          
          # Set outputs
          echo "incident-id=$INCIDENT_ID" >> $GITHUB_OUTPUT
          echo "log-downloaded=true" >> $GITHUB_OUTPUT

      - name: Run Bedrock Analysis
        id: bedrock
        run: |
          echo "üß† Running AI analysis with AWS Bedrock..."
          
          # Create the analysis script using the proven approach from analyze-s3-logs.yml
          cat > analyze-log.sh << 'EOF'
          #!/bin/bash
          
          # Configuration
          AWS_REGION="us-west-1"
          BEDROCK_MODEL="us.anthropic.claude-sonnet-4-20250514-v1:0"
          LOG_FILE="./incident-log.log"
          
          echo "üìñ Reading log file..."
          if [ ! -f "$LOG_FILE" ]; then
            echo "‚ùå Log file not found: $LOG_FILE"
            exit 1
          fi
          
          # Read log content
          LOGS_CONTENT=$(cat "$LOG_FILE")
          LOG_SIZE=$(echo "$LOGS_CONTENT" | wc -c)
          echo "‚úÖ Log content loaded (size: $LOG_SIZE characters)"
          
          # Create analysis prompt using temporary file to avoid JSON escaping issues
          TEMP_PROMPT_FILE="./bedrock_prompt_$$.txt"
          cat > "$TEMP_PROMPT_FILE" << 'PROMPT_EOF'
          You are an expert SRE (Site Reliability Engineer) analyzing a Kubernetes incident. Please analyze the following incident log data and provide a comprehensive incident analysis report.
          
          Please provide your analysis in the following structured format:
          
          ## üìä INCIDENT SUMMARY
          - **Type**: [Brief description of incident type]
          - **Severity**: [LOW|MEDIUM|HIGH|CRITICAL]
          - **Duration**: [Estimated duration]
          - **Affected Services**: [List of affected services]
          
          ## üîç ROOT CAUSE ANALYSIS
          - **Primary Cause**: [Main root cause]
          - **Contributing Factors**: [List of contributing factors]
          
          ## ‚ö° IMMEDIATE FIXES APPLIED
          - [List of immediate actions taken during the incident]
          
          ## üõ°Ô∏è PREVENTIVE MEASURES
          - [List of measures to prevent similar incidents]
          
          ## üìö LESSONS LEARNED
          - [Key lessons from this incident]
          
          ## üéØ RECOMMENDATIONS
          - **Monitoring**: [Monitoring improvements]
          - **Alerting**: [Alerting improvements]
          - **Process**: [Process improvements]
          - **Infrastructure**: [Infrastructure improvements]
          
          Focus on being specific and actionable in your recommendations.
          
          INCIDENT LOG DATA:
          PROMPT_EOF
          
          # Append the log content to the prompt file
          echo "$LOGS_CONTENT" >> "$TEMP_PROMPT_FILE"
          
          # Read the complete prompt
          ANALYSIS_PROMPT=$(cat "$TEMP_PROMPT_FILE")
          
          # Create the Bedrock request payload file using jq to properly escape content
          BEDROCK_PAYLOAD_FILE="./bedrock_payload_$$.json"
          cat > "$BEDROCK_PAYLOAD_FILE" << PAYLOAD_EOF
          {
            "anthropic_version": "bedrock-2023-05-31",
            "messages": [
              {
                "role": "user",
                "content": $(echo "$ANALYSIS_PROMPT" | jq -Rs .)
              }
            ],
            "temperature": 0.3,
            "max_tokens": 4000
          }
          PAYLOAD_EOF
          
          echo "üöÄ Sending request to Bedrock..."
          
          # Call Bedrock using the working method
          if aws bedrock-runtime invoke-model \
            --region "$AWS_REGION" \
            --cli-binary-format raw-in-base64-out \
            --model-id "$BEDROCK_MODEL" \
            --content-type application/json \
            --accept application/json \
            --body "file://$BEDROCK_PAYLOAD_FILE" \
            bedrock-response.json; then
              
              echo "‚úÖ Bedrock analysis completed successfully!"
              
              # Parse and display results
              RESPONSE_CONTENT=$(cat bedrock-response.json)
              
              if command -v jq &> /dev/null; then
                  # Extract the analysis text
                  ANALYSIS_TEXT=$(echo "$RESPONSE_CONTENT" | jq -r '.content[0].text // empty')
                  
                  if [ -n "$ANALYSIS_TEXT" ]; then
                      echo ""
                      echo "üìä INCIDENT ANALYSIS RESULTS"
                      echo "============================"
                      echo "$ANALYSIS_TEXT"
                      echo "============================"
                      echo ""
                      
                      # Save formatted results
                      echo "# ü§ñ AI Incident Analysis Report" > analysis-results.txt
                      echo "" >> analysis-results.txt
                      echo "**Generated:** $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> analysis-results.txt
                      echo "**Model:** $BEDROCK_MODEL" >> analysis-results.txt
                      echo "**Incident ID:** $INCIDENT_ID" >> analysis-results.txt
                      echo "" >> analysis-results.txt
                      echo "$ANALYSIS_TEXT" >> analysis-results.txt
                      
                      echo "‚úÖ Full analysis saved to analysis-results.txt"
                      
                  else
                      echo "‚ùå No response content found"
                      cat bedrock-response.json
                      exit 1
                  fi
              else
                  echo "‚ö†Ô∏è jq not available, showing raw response:"
                  cat bedrock-response.json
                  cat bedrock-response.json > analysis-results.txt
              fi
              
              # Clean up temporary files
              rm -f "$TEMP_PROMPT_FILE" "$BEDROCK_PAYLOAD_FILE"
              
              echo "analysis-completed=true" >> $GITHUB_OUTPUT
              
          else
              echo "‚ùå Bedrock analysis failed"
              echo "analysis-completed=false" >> $GITHUB_OUTPUT
              exit 1
          fi
          EOF
          
          chmod +x analyze-log.sh
          ./analyze-log.sh

      - name: Upload AI Analysis Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: ai-analysis-${{ steps.s3-download.outputs.incident-id }}
          path: |
            bedrock-response.json
            incident-log.log
            analysis-results.txt
            bedrock-payload.json
          retention-days: 30

  generate-summary:
    name: üìã Generate Summary
    runs-on: ubuntu-latest
    needs: [pre-checks, simulate-incident, ai-analysis]
    if: always()
    steps:
      - name: Generate Incident Summary
        run: |
          echo "## üö® Incident Simulation Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìä Incident Details" >> $GITHUB_STEP_SUMMARY
          echo "- **Incident ID:** ${{ needs.simulate-incident.outputs.incident-id || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Type:** ${{ inputs.incident_type }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Duration:** ${{ inputs.duration }} minutes" >> $GITHUB_STEP_SUMMARY
          echo "- **Service URL:** http://${{ needs.pre-checks.outputs.service-url || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **AI Analysis:** ${{ inputs.enable_ai_analysis && '‚úÖ Enabled' || '‚ùå Disabled' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìÅ Artifacts Generated" >> $GITHUB_STEP_SUMMARY
          echo "- **Baseline Logs:** Available in artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- **Incident Logs:** Available in artifacts" >> $GITHUB_STEP_SUMMARY
          if [ "${{ inputs.enable_ai_analysis }}" = "true" ]; then
            echo "- **AI Analysis:** Available in artifacts" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üéØ Next Steps" >> $GITHUB_STEP_SUMMARY
          echo "1. **Review Artifacts:** Download and analyze the generated logs" >> $GITHUB_STEP_SUMMARY
          echo "2. **Check Grafana:** Monitor real-time metrics in Grafana dashboards" >> $GITHUB_STEP_SUMMARY
          echo "3. **Verify Recovery:** Ensure application has returned to normal state" >> $GITHUB_STEP_SUMMARY
          if [ "${{ inputs.enable_ai_analysis }}" = "true" ]; then
            echo "4. **Review AI Analysis:** Check the AI-generated incident analysis and recommendations" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ‚ö†Ô∏è Cleanup" >> $GITHUB_STEP_SUMMARY
          echo "Remember to run the **teardown.yml** workflow when done to avoid AWS charges!" >> $GITHUB_STEP_SUMMARY
