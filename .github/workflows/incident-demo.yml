name: Incident Simulation Demo

on:
  workflow_dispatch:
    inputs:
      incident_type:
        description: 'Type of incident to simulate'
        required: true
        default: 'memory_leak'
        type: choice
        options:
        - memory_leak
        - cpu_stress
        - health_failure
        - all_scenarios
      duration:
        description: 'Duration in minutes'
        required: true
        default: '1'
        type: string
      enable_ai_analysis:
        description: 'Enable AI incident analysis'
        required: false
        default: true
        type: boolean

env:
  AWS_REGION: eu-central-1
  CLUSTER_NAME: sre-incident-demo-cluster
  APP_NAME: sre-demo-app
  NAMESPACE: default
  TF_STATE_BUCKET: sre-terraform-state-${{ github.repository_owner }}
  TF_STATE_KEY: sre-demo/terraform.tfstate

jobs:
  pre-checks:
    name: 🔍 Pre-Flight Checks
    runs-on: ubuntu-latest
    outputs:
      service-url: ${{ steps.service.outputs.service-url }}
      cluster-status: ${{ steps.cluster.outputs.status }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Check EKS Cluster
        id: cluster
        run: |
          echo "🔍 Checking EKS cluster status..."
          if aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.status' --output text | grep -q "ACTIVE"; then
            echo "status=active" >> $GITHUB_OUTPUT
            echo "✅ EKS cluster is active"
          else
            echo "status=inactive" >> $GITHUB_OUTPUT
            echo "❌ EKS cluster is not active"
            exit 1
          fi

      - name: Configure kubectl
        run: |
          echo "⚙️ Configuring kubectl..."
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}
          kubectl cluster-info

      - name: Check Application Status
        id: service
        run: |
          echo "🔍 Checking application status..."
          
          # Check if pods are running
          if ! kubectl get pods -l app.kubernetes.io/name=${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} --no-headers | grep -q "Running"; then
            echo "❌ Application pods are not running"
            kubectl get pods -l app.kubernetes.io/name=${{ env.APP_NAME }} -n ${{ env.NAMESPACE }}
            exit 1
          fi
          
          # Get service URL
          SERVICE_URL=$(kubectl get svc ${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ -z "$SERVICE_URL" ]; then
            SERVICE_URL=$(kubectl get svc ${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
          fi
          
          if [ -n "$SERVICE_URL" ]; then
            echo "service-url=$SERVICE_URL" >> $GITHUB_OUTPUT
            echo "✅ Service URL: http://$SERVICE_URL"
            
            # Test application health
            if curl -s --max-time 10 "http://$SERVICE_URL/health" > /dev/null; then
              echo "✅ Application is responding"
            else
              echo "⚠️ Application may not be fully ready"
            fi
          else
            echo "❌ Service URL not available"
            exit 1
          fi

  baseline-metrics:
    name: 📊 Capture Baseline Metrics
    runs-on: ubuntu-latest
    needs: pre-checks
    outputs:
      baseline-captured: ${{ steps.metrics.outputs.captured }}
      incident-id: ${{ steps.metrics.outputs.incident-id }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: Capture Baseline Metrics
        id: metrics
        run: |
          echo "📊 Capturing baseline metrics..."
          
          # Create incident directory
          INCIDENT_ID="gh-incident-$(date +%Y%m%d-%H%M%S)"
          mkdir -p /tmp/incident-logs
          
          echo "🆔 Incident ID: $INCIDENT_ID"
          echo "incident-id=$INCIDENT_ID" >> $GITHUB_OUTPUT
          
          # Capture baseline state
          echo "=== BASELINE METRICS ===" > /tmp/incident-logs/baseline-$INCIDENT_ID.log
          echo "Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          echo "Incident ID: $INCIDENT_ID" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          echo "" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          
          # Pod status
          echo "=== POD STATUS ===" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          kubectl get pods -l app.kubernetes.io/name=${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} -o wide >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          echo "" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          
          # Resource usage
          echo "=== RESOURCE USAGE ===" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          kubectl top pods -l app.kubernetes.io/name=${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} >> /tmp/incident-logs/baseline-$INCIDENT_ID.log 2>&1 || echo "Metrics server not available" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          echo "" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          
          # Service status
          echo "=== SERVICE STATUS ===" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          kubectl get svc ${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} -o wide >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          echo "" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          
          # HPA status
          echo "=== HPA STATUS ===" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          kubectl get hpa -n ${{ env.NAMESPACE }} >> /tmp/incident-logs/baseline-$INCIDENT_ID.log 2>&1 || echo "HPA not found" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          echo "" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          
          # Application health
          echo "=== APPLICATION HEALTH ===" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          curl -s --max-time 10 "http://${{ needs.pre-checks.outputs.service-url }}/health" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log 2>&1 || echo "Health check failed" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          echo "" >> /tmp/incident-logs/baseline-$INCIDENT_ID.log
          
          echo "captured=true" >> $GITHUB_OUTPUT
          echo "✅ Baseline metrics captured"

      - name: Upload Baseline Logs
        uses: actions/upload-artifact@v4
        with:
          name: baseline-logs
          path: /tmp/incident-logs/
          retention-days: 7

  simulate-incident:
    name: 🚨 Simulate Incident
    runs-on: ubuntu-latest
    needs: [pre-checks, baseline-metrics]
    outputs:
      incident-id: ${{ steps.incident.outputs.incident-id }}
      s3-incident-url: ${{ steps.s3-upload.outputs.s3-incident-url }}
      s3-bucket: ${{ steps.s3-upload.outputs.s3-bucket }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: Download Baseline Logs
        uses: actions/download-artifact@v4
        with:
          name: baseline-logs
          path: /tmp/incident-logs/

      - name: Simulate Incident
        id: incident
        run: |
          echo "🚨 Starting incident simulation..."
          
          SERVICE_URL="${{ needs.pre-checks.outputs.service-url }}"
          INCIDENT_TYPE="${{ inputs.incident_type }}"
          DURATION="${{ inputs.duration }}"
          INCIDENT_ID="${{ needs.baseline-metrics.outputs.incident-id }}"
          
          echo "🆔 Incident ID: $INCIDENT_ID"
          echo "🎯 Incident Type: $INCIDENT_TYPE"
          echo "⏱️ Duration: $DURATION minutes"
          echo "🌐 Service URL: http://$SERVICE_URL"
          
          echo "incident-id=$INCIDENT_ID" >> $GITHUB_OUTPUT
          
          # Create incident log directory
          mkdir -p /tmp/incident-logs
          
          # Start incident logging (create log file in expected format)
          echo "=== INCIDENT SIMULATION LOG ===" > /tmp/incident-logs/incident-$INCIDENT_ID.log
          echo "Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
          echo "Incident ID: $INCIDENT_ID" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
          echo "Incident Type: $INCIDENT_TYPE" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
          echo "Duration: $DURATION minutes" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
          echo "Service URL: http://$SERVICE_URL" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
          echo "" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
          
          # Function to trigger incident
          trigger_incident() {
            local type=$1
            echo "🚨 Triggering $type incident..." >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            
            case $type in
              "memory_leak")
                echo "💾 Enabling memory leak simulation..."
                curl -X POST "http://$SERVICE_URL/api/memory-leak" \
                  -H "Content-Type: application/json" \
                  -d '{"enable": true}' \
                  >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1
                ;;
              "cpu_stress")
                echo "🔥 Enabling CPU stress simulation..."
                curl -X POST "http://$SERVICE_URL/api/cpu-stress" \
                  -H "Content-Type: application/json" \
                  -d '{"enable": true}' \
                  >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1
                ;;
              "health_failure")
                echo "💔 Enabling health failure simulation..."
                curl -X POST "http://$SERVICE_URL/api/failure-mode" \
                  -H "Content-Type: application/json" \
                  -d '{"mode": "health_failure"}' \
                  >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1
                ;;
            esac
          }
          
          # Function to stop incident
          stop_incident() {
            echo "🛑 Stopping incident simulation..." >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            
            # Stop all simulations
            curl -X POST "http://$SERVICE_URL/api/memory-leak" \
              -H "Content-Type: application/json" \
              -d '{"enable": false}' \
              >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1
            
            curl -X POST "http://$SERVICE_URL/api/cpu-stress" \
              -H "Content-Type: application/json" \
              -d '{"enable": false}' \
              >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1
            
            curl -X POST "http://$SERVICE_URL/api/failure-mode" \
              -H "Content-Type: application/json" \
              -d '{"mode": "none"}' \
              >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1
          }
          
          # Function to collect metrics
          collect_metrics() {
            local timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)
            echo "📊 Collecting metrics at $timestamp..." >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            
            # Pod status
            echo "=== POD STATUS ($timestamp) ===" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            kubectl get pods -l app.kubernetes.io/name=${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} -o wide >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            
            # Resource usage
            echo "=== RESOURCE USAGE ($timestamp) ===" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            kubectl top pods -l app.kubernetes.io/name=${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1 || echo "Metrics server not available" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            
            # Recent events
            echo "=== RECENT EVENTS ($timestamp) ===" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            kubectl get events --sort-by='.lastTimestamp' -n ${{ env.NAMESPACE }} --field-selector involvedObject.name=${{ env.APP_NAME }} | tail -10 >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1
            
            # Application health
            echo "=== APPLICATION HEALTH ($timestamp) ===" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            curl -s --max-time 5 "http://$SERVICE_URL/health" >> /tmp/incident-logs/incident-$INCIDENT_ID.log 2>&1 || echo "Health check failed" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
            
            echo "" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
          }
          
          # Trigger incidents based on type
          if [ "$INCIDENT_TYPE" = "all_scenarios" ]; then
            echo "🎭 Running all incident scenarios..."
            for scenario in "memory_leak" "cpu_stress" "health_failure"; do
              echo "--- Starting $scenario scenario ---" >> /tmp/incident-logs/incident-$INCIDENT_ID.log
              trigger_incident $scenario
              sleep 30
              collect_metrics
              stop_incident
              sleep 10
            done
          else
            trigger_incident $INCIDENT_TYPE
            
            # Monitor for specified duration
            DURATION_SECONDS=$((DURATION * 60))
            INTERVAL=30
            ITERATIONS=$((DURATION_SECONDS / INTERVAL))
            
            echo "📊 Monitoring for $DURATION minutes (collecting metrics every ${INTERVAL}s)..."
            for i in $(seq 1 $ITERATIONS); do
              echo "📊 Metrics collection $i/$ITERATIONS"
              collect_metrics
              
              if [ $i -lt $ITERATIONS ]; then
                sleep $INTERVAL
              fi
            done
            
            # Stop the incident
            stop_incident
          fi
          
          # Final metrics collection
          echo "📊 Collecting final metrics..."
          sleep 10
          collect_metrics
          
          echo "✅ Incident simulation completed"

      - name: Collect Pod Logs
        run: |
          echo "📋 Collecting pod logs..."
          INCIDENT_ID="${{ steps.incident.outputs.incident-id }}"
          
          # Get pod logs
          echo "=== APPLICATION LOGS ===" > /tmp/incident-logs/pod-logs-$INCIDENT_ID.log
          kubectl logs -l app.kubernetes.io/name=${{ env.APP_NAME }} -n ${{ env.NAMESPACE }} --tail=500 >> /tmp/incident-logs/pod-logs-$INCIDENT_ID.log 2>&1 || echo "Failed to collect pod logs" >> /tmp/incident-logs/pod-logs-$INCIDENT_ID.log
          
          echo "✅ Pod logs collected"

      - name: Upload to S3 (if AI analysis enabled)
        id: s3-upload
        if: ${{ inputs.enable_ai_analysis }}
        run: |
          echo "☁️ Uploading logs to S3 for AI analysis..."
          INCIDENT_ID="${{ steps.incident.outputs.incident-id }}"
          
          # Get AWS account ID
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          S3_BUCKET="sre-incident-demo-incident-logs-${AWS_ACCOUNT_ID}"
          
          # Create S3 bucket if it doesn't exist
          if ! aws s3api head-bucket --bucket "$S3_BUCKET" 2>/dev/null; then
            echo "📦 Creating S3 bucket for incident logs..."
            aws s3api create-bucket \
              --bucket "$S3_BUCKET" \
              --region ${{ env.AWS_REGION }} \
              --create-bucket-configuration LocationConstraint=${{ env.AWS_REGION }}
            
            # Enable versioning and encryption
            aws s3api put-bucket-versioning --bucket "$S3_BUCKET" --versioning-configuration Status=Enabled
            aws s3api put-bucket-encryption --bucket "$S3_BUCKET" --server-side-encryption-configuration '{
              "Rules": [{
                "ApplyServerSideEncryptionByDefault": {"SSEAlgorithm": "AES256"},
                "BucketKeyEnabled": true
              }]
            }'
          fi
          
          # Upload logs (check files exist first)
          echo "🔍 Checking available log files..."
          ls -la /tmp/incident-logs/ || echo "No log files found"
          
          echo "📤 Uploading logs to S3..."
          if [ -f "/tmp/incident-logs/incident-$INCIDENT_ID.log" ]; then
            aws s3 cp /tmp/incident-logs/incident-$INCIDENT_ID.log s3://$S3_BUCKET/incidents/$INCIDENT_ID/incident.log
            echo "✅ Incident log uploaded"
          else
            echo "⚠️ Incident log not found: /tmp/incident-logs/incident-$INCIDENT_ID.log"
          fi
          
          if [ -f "/tmp/incident-logs/baseline-$INCIDENT_ID.log" ]; then
            aws s3 cp /tmp/incident-logs/baseline-$INCIDENT_ID.log s3://$S3_BUCKET/incidents/$INCIDENT_ID/baseline.log
            echo "✅ Baseline log uploaded"
          else
            echo "⚠️ Baseline log not found: /tmp/incident-logs/baseline-$INCIDENT_ID.log"
          fi
          
          if [ -f "/tmp/incident-logs/pod-logs-$INCIDENT_ID.log" ]; then
            aws s3 cp /tmp/incident-logs/pod-logs-$INCIDENT_ID.log s3://$S3_BUCKET/incidents/$INCIDENT_ID/pod-logs.log
            echo "✅ Pod logs uploaded"
          else
            echo "⚠️ Pod logs not found: /tmp/incident-logs/pod-logs-$INCIDENT_ID.log"
          fi
          
          # Generate S3 URLs for AI analysis
          S3_INCIDENT_URL="https://$S3_BUCKET.s3.${{ env.AWS_REGION }}.amazonaws.com/incidents/$INCIDENT_ID/incident.log"
          
          echo "s3-bucket=$S3_BUCKET" >> $GITHUB_OUTPUT
          echo "s3-incident-url=$S3_INCIDENT_URL" >> $GITHUB_OUTPUT
          echo "✅ Logs uploaded to S3: s3://$S3_BUCKET/incidents/$INCIDENT_ID/"

      - name: Upload Incident Logs Artifact
        id: upload
        uses: actions/upload-artifact@v4
        with:
          name: incident-logs-${{ steps.incident.outputs.incident-id }}
          path: /tmp/incident-logs/
          retention-days: 7

  ai-analysis:
    name: 🤖 AI Incident Analysis
    runs-on: ubuntu-latest
    needs: [simulate-incident]
    if: ${{ inputs.enable_ai_analysis }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-1  # Bedrock region

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq curl unzip

      - name: Extract S3 info and download log
        id: s3-download
        run: |
          echo "🤖 Running AI incident analysis..."
          INCIDENT_ID="${{ needs.simulate-incident.outputs.incident-id }}"
          S3_INCIDENT_URL="${{ needs.simulate-incident.outputs.s3-incident-url }}"
          S3_BUCKET="${{ needs.simulate-incident.outputs.s3-bucket }}"
          
          echo "🔍 Debug info:"
          echo "  Incident ID: '$INCIDENT_ID'"
          echo "  S3 URL: '$S3_INCIDENT_URL'"
          echo "  S3 Bucket: '$S3_BUCKET'"
          
          if [ -z "$S3_INCIDENT_URL" ]; then
            echo "❌ ERROR: Missing S3 incident URL from simulate-incident job!"
            echo "Available outputs:"
            echo "  incident-id: '${{ needs.simulate-incident.outputs.incident-id }}'"
            echo "  s3-incident-url: '${{ needs.simulate-incident.outputs.s3-incident-url }}'"
            echo "  s3-bucket: '${{ needs.simulate-incident.outputs.s3-bucket }}'"
            exit 1
          fi
          
          if [ -z "$INCIDENT_ID" ]; then
            echo "⚠️ WARNING: Incident ID is empty, using timestamp fallback"
            INCIDENT_ID="fallback-$(date +%Y%m%d-%H%M%S)"
          fi
          
          # Extract S3 bucket and key from URL
          if [[ "$S3_INCIDENT_URL" == https://*.s3.*.amazonaws.com/* ]]; then
            BUCKET=$(echo "$S3_INCIDENT_URL" | sed 's|https://||' | cut -d'.' -f1)
            KEY=$(echo "$S3_INCIDENT_URL" | sed 's|https://[^/]*/||')
          else
            echo "❌ ERROR: Invalid S3 URL format: $S3_INCIDENT_URL"
            exit 1
          fi
          
          echo "📥 Downloading incident log from S3..."
          echo "  Bucket: $BUCKET"
          echo "  Key: $KEY"
          
          aws s3 cp "s3://$BUCKET/$KEY" ./incident-log.log
          
          echo "📊 Log file info:"
          echo "  Size: $(wc -c < incident-log.log) bytes"
          echo "  Lines: $(wc -l < incident-log.log)"
          
          # Set outputs
          echo "incident-id=$INCIDENT_ID" >> $GITHUB_OUTPUT
          echo "log-downloaded=true" >> $GITHUB_OUTPUT

      - name: Run Bedrock Analysis
        id: bedrock
        run: |
          echo "🧠 Running AI analysis with AWS Bedrock..."
          
          # Create the analysis script inline (based on analyze-s3-logs.yml approach)
          cat > analyze-log.sh << 'EOF'
          #!/bin/bash
          
          # Configuration
          BEDROCK_MODEL="us.anthropic.claude-sonnet-4-20250514-v1:0"
          LOG_FILE="incident-log.log"
          
          echo "📖 Reading log file..."
          if [ ! -f "$LOG_FILE" ]; then
            echo "❌ Log file not found: $LOG_FILE"
            exit 1
          fi
          
          LOG_CONTENT=$(cat "$LOG_FILE")
          echo "✅ Log content loaded ($(echo "$LOG_CONTENT" | wc -c) characters)"
          
          # Create Bedrock payload
          cat > bedrock-payload.json << EOL
          {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 4000,
            "messages": [
              {
                "role": "user",
                "content": "You are an expert Site Reliability Engineer (SRE) and incident response specialist. Analyze the following incident log and provide a comprehensive analysis including:\n\n1. **Incident Summary**: Brief overview of what happened\n2. **Root Cause Analysis**: Identify the primary cause\n3. **Timeline**: Key events and timestamps\n4. **Impact Assessment**: What was affected and severity\n5. **Resolution Steps**: What was done to resolve it\n6. **Prevention Recommendations**: How to prevent similar incidents\n7. **Monitoring Improvements**: Suggested alerts or metrics\n\nIncident Log:\n\`\`\`\n$LOG_CONTENT\n\`\`\`\n\nProvide your analysis in a structured, actionable format suitable for an incident report."
              }
            ]
          }
          EOL
          
          echo "🚀 Sending request to Bedrock..."
          aws bedrock-runtime invoke-model \
            --model-id "$BEDROCK_MODEL" \
            --content-type application/json \
            --accept application/json \
            --body file://bedrock-payload.json \
            bedrock-response.json
          
          if [ $? -eq 0 ]; then
            echo "✅ Bedrock analysis completed successfully"
            
            # Extract and display the analysis
            echo "📋 AI Analysis Results:"
            echo "========================"
            jq -r '.content[0].text' bedrock-response.json
            echo "========================"
            
            # Save formatted results
            echo "# 🤖 AI Incident Analysis Report" > analysis-results.txt
            echo "" >> analysis-results.txt
            echo "**Generated:** $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> analysis-results.txt
            echo "**Model:** $BEDROCK_MODEL" >> analysis-results.txt
            echo "" >> analysis-results.txt
            jq -r '.content[0].text' bedrock-response.json >> analysis-results.txt
            
            echo "analysis-completed=true" >> $GITHUB_OUTPUT
          else
            echo "❌ Bedrock analysis failed"
            echo "analysis-completed=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          EOF
          
          chmod +x analyze-log.sh
          ./analyze-log.sh

      - name: Upload AI Analysis Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: ai-analysis-${{ steps.s3-download.outputs.incident-id }}
          path: |
            bedrock-response.json
            incident-log.log
            analysis-results.txt
            bedrock-payload.json
          retention-days: 30

  generate-summary:
    name: 📋 Generate Summary
    runs-on: ubuntu-latest
    needs: [pre-checks, simulate-incident, ai-analysis]
    if: always()
    steps:
      - name: Generate Incident Summary
        run: |
          echo "## 🚨 Incident Simulation Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 📊 Incident Details" >> $GITHUB_STEP_SUMMARY
          echo "- **Incident ID:** ${{ needs.simulate-incident.outputs.incident-id || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Type:** ${{ inputs.incident_type }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Duration:** ${{ inputs.duration }} minutes" >> $GITHUB_STEP_SUMMARY
          echo "- **Service URL:** http://${{ needs.pre-checks.outputs.service-url || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **AI Analysis:** ${{ inputs.enable_ai_analysis && '✅ Enabled' || '❌ Disabled' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 📁 Artifacts Generated" >> $GITHUB_STEP_SUMMARY
          echo "- **Baseline Logs:** Available in artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- **Incident Logs:** Available in artifacts" >> $GITHUB_STEP_SUMMARY
          if [ "${{ inputs.enable_ai_analysis }}" = "true" ]; then
            echo "- **AI Analysis:** Available in artifacts" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 🎯 Next Steps" >> $GITHUB_STEP_SUMMARY
          echo "1. **Review Artifacts:** Download and analyze the generated logs" >> $GITHUB_STEP_SUMMARY
          echo "2. **Check Grafana:** Monitor real-time metrics in Grafana dashboards" >> $GITHUB_STEP_SUMMARY
          echo "3. **Verify Recovery:** Ensure application has returned to normal state" >> $GITHUB_STEP_SUMMARY
          if [ "${{ inputs.enable_ai_analysis }}" = "true" ]; then
            echo "4. **Review AI Analysis:** Check the AI-generated incident analysis and recommendations" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ⚠️ Cleanup" >> $GITHUB_STEP_SUMMARY
          echo "Remember to run the **teardown.yml** workflow when done to avoid AWS charges!" >> $GITHUB_STEP_SUMMARY
